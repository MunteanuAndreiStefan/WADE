<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
    <title>Fake news detection</title>
    <link rel="stylesheet" href="css/scholarly.min.css">
    <script src="js/scholarly.min.js"></script>
</head>

<body prefix="schema: http://schema.org">
    <header>
        <div class="banner">
            <img src="scholarly-html.svg" width="227" height="50" alt="Scholarly HTML logo">
            <div class="status">Community Draft</div>
        </div>
        <h1>Fake News Detection</h1>
    </header>
    <div role="contentinfo">
        <dl>
            <dt>Authors:</dt>
            <dd>Victor Vlad </dd>
            <dd>Munteanu Andrei-Ștefan</dd>
			<dd>Pantelemon Victor-Stefan</dd>
			
        </dl>
    </div>

    <section typeof="sa:Abstract" id="abstract" role="doc-abstract">
        <h2>Intoduction</h2>
        <p>
            The project aims to detect fake news from the Twitter platform using modern technologies such as machine learning and SPARQL to verify the credibility of the content of a news story. The project is divided into several modules that will be described below.
        </p>
    </section>
	
	<section typeof="sa:Problem" id="problem" role="doc-problem">
        <h2>Problem solved</h2>
        <p>
            The problem consists into problems which are visible every day when we are navigating on twitter. That problem is that we deal with lots of fake news and fakes user. Our scope with this project is to determine the probability of a user to be fake or not. 
        </p>
		<p>
            Another problem would be that there aren’t almost any real time detection tools which could do this for a home user which is browsing the web. Most of the tools on the web need an insane amount of time to do semantic analysis over that topic or to analyze the comments, reactions and other types of forms a user has over a unique post. This kind of tools aren’t actually a good tool for fake news which are starting from multiple sources at the same time making a web explosion of fake news over twitter especially.
        </p>
    </section>
	
	<section typeof="sa:SoftwareRequirements" id="Requirements" role="softwareRequirements">
        <h2>Functional requirements</h2>
        <p>
            All information will be collected from twitter via api (posts, user information (number of posts, followers, account creation date, etc.). We will also have user profiles built (characterized by details about the user, user activity, reactions of other users to what he does on the network, etc.) All this information will be used in order to have:
        </p>
		<p> - A metric will be proposed to calculate the credibility of a post</p>
		<p> - A metric will be proposed for credibility of a user</p>
		<p>
            All the available metainformation should be taken into consideration. Also, we should have a list of fake users centralized in order to get a better response, after the front-end is putting the user in a category. Resources should be centralized in order to get faster responses for already processed news. Resources will be used outside the Twitter network to validate the information (Google, blogs, newspapers, etc.) via a crawler in order to see if the information from the post/user and their metric can be considered trustful. Fronted will be able to show extra information about metrics also it should be able to edit the strictness level for marking news with the possibility of being fake or not. Fronted will show analyzed resources. User will also be able to mark a post as fake manually.
        </p>
    </section>

    <section id="arhitecture">
        <h2>Architecture of the Web Application</h2>
        <p>
			Our fronted is made via a chrome extension which allow us to add functionality to the Chrome web browser without diving deeply into native code. Fake news browser extension was built with core technologies that web developers are very familiar with - HTML, CSS, and JavaScript. The only thing different from classic web pages are functionality to Chrome through some of the JavaScript APIs that Chrome exposes. Fake news extension was also created to work only on certain pages through the use of Page Actions, it can run code in the background using Background Pages, and can even modify an existing page loaded in the browser using Content Scripts. Also, a manifest.json file was created to tell Chrome important information about extension, like its name and which permissions it needs. Also, the extension is made of different, but cohesive, components. Components include background scripts, content scripts, UI elements and various logic files. The following modules that are linked together to create a service-oriented architecture that can be scaled in the future.
		</p>
            <img src="images/Diag_2.png" height="580" width="805" /> 
			
			<p>
			Our backend was composed via multiple tentatives and we tried to include all classes from advanced software engineering techniques into it, so it may be considered our playground. We have started with the methodology being a model driven development one. Then with the architecture n tier pattern which was done with model first entity framework and database first with entity framework core in order to stay in our methodology grounds. The backend is a restful api which respects all SOLID principles and it is secured via authentication and authorization having multiple keys and rights. It is also secured having a token for each user generated. The deployment is done into Azure cloud due the fact that is a very friendly environment which supports also CI/CD in order for us to be able to build and deploy faster. Also, we were thinking to go from GitHub to the VSTS platform due the fact that is easier to follow all tasks assigned with repos and with pipelines for CI/CD and test plans. Our architecture is service oriented and we use in our project python, JavaScript and C#.
			You can find more details about this into this BPMN.</p>
            <img src="images/Diag_1.png" height="733" width="733" style="margin-top: 20px;" /></p>

    </section>

    <section typeof="sa:SoftwareRequirements" id="dataFlows" role="softwareRequirements">
        <h2>Non-Functional Requirements</h2>
        <ul>
            <li>Cache centralized system for all the users</li>
            <li>Nothing on security side for the final product due it's created only for academic purposes</li>
            <li>The application will run for chrome and firefox</li>
            <li>Level of trust per user, calculated with a Bayes/ML/Statistical methods over semantic or non semantic data</li>
            <li>Take advantage of posts metadata</li>
            <li>"Layers of trust" in order to say as fast as possible if a post is fake or not</li>
            <li>Parser with BS4 in order to get information from trusted websites</li>
            <li>Implement a crawler maybe with search engines in order to find credible sources</li>
        </ul>
    </section>

    <section id="modules">
        <h2>Modules</h2>
        <section id="caching">
            <h3> Cahing </h3>
            <p>
                The backend cache module is responsible for the delivery of the cached/processed data to the frontend. The browser extensions is sending various requests to the cache module performing CRUD operations on the entities, mainly read requests on the news article entity. The news article instances are uniquely identified by their uri. For each article that cannot be found in the cache the caching module is calling the processing module storing the result in the cache.
            </p>
        </section>
        <section id="processing">
            <h3> Processing </h3>
            <p>
                Process Unit is a server that receives requests with twitter URL Based on a tweet, we can extract the text content and query Google with the tokenized (cleaned up) text. We find articles from which we extract the body and compare semantically with the tweet. We average with all the results found and that's the score.
            </p>
        </section>
        <section id="scrapper">
            <h3> Article Scraper </h3>
            <p>
                The crawling service is a simple python script which is able to crawl the google with the scope of finding articles which could contain interest keys for us. Then those links are saved and they are opened in parallel in order to crawl them as well and see if that information is or not of any help in our process unit. We also use google api.
            </p>
        </section>
		<section id="twitterScrapper">
            <h3> Twitter Scraper </h3>
            <p>
                The scrapper for twitter (* requires developer account for API keys, * runs non-stop by searching for certain keywords) is able to save data in mongo collections about user, text, date of post, number of likes, retweets, if it was posted as a response, but also data about user. It’s exporting from mongo a json which is then used in a Redis queue with the scope of being processed with multithreading. On its part we are also using a Bayes and other ML algorithms to see if the tags like emails or other things match with our keywords needed. Also, this API allows us to give a credibility log. Score over the users in terms of fakeness.
            </p>
        </section>
		<section id="moreInfoModues">
            <h3> Information about modules implementation </h3>
            <p>Some important things that didn’t come along as expected with this module it would be that: </p>
			<ul>
				<li> It did not come out to extract semantic relationships from the text. It is possible to classify words into speech / sentence parts but we couldn’t do it in time to use pairs to check ontologies.</li>
				<li> We tried instead to compare the whole article with the tweet, to take each sentence and compare it, but it takes a long time after that. We could use a heuristic with which to exclude sentences that we are not interested in to, at least that was our final resolution in order to make it faster.</li>
			</ul>
			<p>In order to implement this, we used modules for:</p>
			<ul>
				<li>Tokenization, lemmatization, stop words – nltk</li>
				<li>Semantic analyze – genism and nltk</li>
				<li>Semantic querry – SparQl</li>
				<li>Hosting – Azure services</li>
				<li>Backend – EF& .NET Core 3.0</li>
				<li>Tweepy – twitter api</li>
				<li>Extract information from articles – newspaper</li>
				<li>Calculate things – numpy</li>
				<li>Server for module – flask</li>
				<li>Requests to external sources - Req.py</li>
				<li>Scraping pages for information - BeautifulSoup4</li>
			</ul>
			<p>All our presented modules are designed with aspect-oriented programming and monitoring oriented programming in mind, also for critical paths we developed functional testing (unit testing) and from non functional perspective we created a collection in postman. In order to have the project in the current form we did a lot of refactoring. Now almost everything is decupled and can be replaced with anything.</p>
			<p>Comparing our solution with others it isn’t the best from our minimal tests till now, from the point of metrics resulted not architecture, an attached annexes which will come will describe this study with more statistics, but from our tests we can only get high confidence of a fake news only when we have some more information about it and the text is on other safe sources already.</p>
        </section>
    </section>





    <section id="data">
        <h2>Input/output Data Formats</h2>
        <section id="model">
            <h3>Entity Framework Model</h3>
            <p>
                The database was generated through entity framework using model first technique. Entities were then generated from the database using entity framework core. The CRUD operations are applied to them from API through the service layer.
				Also we can generate entities doing migrating.
            </p>
            <img src="images/Diag_3.png" height="452" width="525" />
        </section>

        <section id="model-1">
            <h3>Swagger schema</h3>
            <p> Representation of the model used by the API. For more details <a href="https://fakenewsdetection.azurewebsites.net/swagger/index.html">here.</a> </p>
            <p> Application user model schema:</p>
			<pre><code>
		"ApplicationUserViewModel": {
		"required": [
		  "username"
		],
		"type": "object",
		"properties": {
		  "id": {
			"type": "integer",
			"format": "int32",
			"nullable": true
		  },
		  "username": {
			"type": "string",
			"nullable": true
		  },
		  "credibilityScore": {
			"type": "integer",
			"format": "int32",
			"nullable": true
		  }
		},
		"additionalProperties": false
	    }
			</code></pre>
			<p> Application user filtering schema:</p>
			<pre><code>
		"ApplicationUserFilterViewModel": {
		"type": "object",
		"properties": {
		  "filterByUsername": {
			"type": "string",
			"nullable": true
		  }
		},
		"additionalProperties": false
		}
			</code></pre>
			<p> Twitter article news schema:</p>
            <pre><code>
		"NewsArticleViewModel": {
		"required": [
		  "credibilityScore",
		  "source",
		  "userId"
		],
		"type": "object",
		"properties": {
		  "id": {
			"type": "integer",
			"format": "int32",
			"nullable": true
		  },
		  "source": {
			"type": "string",
			"nullable": true
		  },
		  "credibilityScore": {
			"type": "integer",
			"format": "int32",
			"nullable": true
		  },
		  "userId": {
			"type": "integer",
			"format": "int32"
		  }
		},
		"additionalProperties": false
		}
			</code></pre>
			<p> Twitter article filtering schema:</p>
			<pre><code>
		"NewsArticleFilterViewModel": {
		"type": "object",
		"properties": {
		  "filterBySource": {
			"type": "string",
			"nullable": true
		  },
		  "filterByUserId": {
			"type": "integer",
			"format": "int32",
			"nullable": true
		  },
		  "filterByUrls": {
			"type": "array",
			"items": {
			  "type": "string"
			},
			"nullable": true
		  }
		},
		"additionalProperties": false
		}
			</code></pre>
			<p> Twitter user schema:</p>
			<pre><code>			
		"TwitterUserViewModel":{
		   "required":[
			  "username"
		   ],
		   "type":"object",
		   "properties":{
			  "id":{
				 "type":"integer",
				 "format":"int32",
				 "nullable":true
			  },
			  "username":{
				 "type":"string",
				 "nullable":true
			  },
			  "credibilityScore":{
				 "type":"integer",
				 "format":"int32",
				 "nullable":true
			  }
		   },
		   "additionalProperties":false
		}
			</code></pre>
			<p> Twitter user filtering schema:</p>
			<code><pre>
		"TwitterUserFilterViewModel": {
		"type": "object",
		"properties": {
		  "filterByUsername": {
			"type": "string",
			"nullable": true
		  }
		},
		"additionalProperties": false
		}
			</code></pre>
        </section>

        <section id="model-2">
            <h3>Twitter api call</h3>
            <p> Representation of a response sent by twitter api, more details <a href="https://developer.twitter.com/en/docs/tweets/curate-a-collection/overview/response_structures">here.</a> </p>
            <pre><code>
{
"created_at": "Thu Oct 10 13:47:05 +0000 2019",
"id": 1182291461934583813,
"id_str": "1182291461934583813",
"text": "@ZoltyqOfficial @FloridaMichaelM Impeachable conduct is whatever the American people say it is. Donald Trump Boss b\u2026 https://t.co/BYDWIqP4dj",
"truncated": true,
"entities": {
	"hashtags": [],
	"symbols": [],
	"user_mentions": [
		{
			"screen_name": "ZoltyqOfficial",
			"name": "MessengerPigeon // Matthew",
			"id": 4302251596,
			"id_str": "4302251596",
			"indices": [
				0,
				15
			]
		},
		{
			"screen_name": "FloridaMichaelM",
			"name": "Michael Morris",
			"id": 3237186386,
			"id_str": "3237186386",
			"indices": [
				16,
				32
			]
		}
	],
	"urls": [
		{
			"url": "https://t.co/BYDWIqP4dj",
			"expanded_url": "https://twitter.com/i/web/status/1182291461934583813",
			"display_url": "twitter.com/i/web/status/1\u2026",
			"indices": [
				117,
				140
			]
		}
	]
},
"source": "<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>",
"in_reply_to_status_id": 1182290046881325056,
"in_reply_to_status_id_str": "1182290046881325056",
"in_reply_to_user_id": 4302251596,
"in_reply_to_user_id_str": "4302251596",
"in_reply_to_screen_name": "ZoltyqOfficial",
"user": {
	"id": 932315684,
	"id_str": "932315684",
	"name": "Aquamayne",
	"screen_name": "Texasinplay",
	"location": "",
	"description": "",
	"url": null,
	"entities": {
		"description": {
			"urls": []
		}
	},
	"protected": false,
	"followers_count": 36,
	"friends_count": 109,
	"listed_count": 1,
	"created_at": "Wed Nov 07 15:35:40 +0000 2012",
	"favourites_count": 2436,
	"utc_offset": null,
	"time_zone": null,
	"geo_enabled": false,
	"verified": false,
	"statuses_count": 259,
	"lang": null,
	"contributors_enabled": false,
	"is_translator": false,
	"is_translation_enabled": false,
	"profile_background_color": "C0DEED",
	"profile_background_image_url": "http://abs.twimg.com/images/themes/theme1/bg.png",
	"profile_background_image_url_https": "https://abs.twimg.com/images/themes/theme1/bg.png",
	"profile_background_tile": false,
	"profile_image_url": "http://pbs.twimg.com/profile_images/1166122176329736192/z9YkVO3E_normal.jpg",
	"profile_image_url_https": "https://pbs.twimg.com/profile_images/1166122176329736192/z9YkVO3E_normal.jpg",
	"profile_banner_url": "https://pbs.twimg.com/profile_banners/932315684/1566860234",
	"profile_link_color": "1DA1F2",
	"profile_sidebar_border_color": "C0DEED",
	"profile_sidebar_fill_color": "DDEEF6",
	"profile_text_color": "333333",
	"profile_use_background_image": true,
	"has_extended_profile": false,
	"default_profile": true,
	"default_profile_image": false,
	"following": false,
	"follow_request_sent": false,
	"notifications": false,
	"translator_type": "none"
},
"geo": null,
"coordinates": null,
"place": null,
"contributors": null,
"is_quote_status": false,
"retweet_count": 0,
"favorite_count": 0,
"favorited": false,
"retweeted": false,
"lang": "en"
}
			</code></pre>
        </section>

    </section>

    <section id="QoS">
        <h2>Quality of service</h2>
        <p>We identified 2 metrics:</p>

        <section id="responseTime">
            <h3>Response Time</h3>
            <ul>
                <li>With application stopped and tweet not in db: avg. 1m 18s</li>
				<li>With application stopped and tweet exists: avg. 11s</li>
				<li>With application started and process unit stopped: avg. 52s</li>
                <li>With application started and tweet in db: avg. 126ms</li>
				<li>With application started and tweet not in db: avg. 9s</li>
				<li>Restart time api: avg. 11s</li>
				<li>Restart time process unit: avg. 51s</li>
				<li>Number of requests that may not receive answer: 1.3%</li>
            </ul>
        </section>
		
		<section id="Credibility">
            <h3>Credibility score</h3>
            <ul>
                <li>User credibility marking: 53% hits on tested subjects</li>
				<li>Posts credibility: 65% hits on tested tweets</li>
            </ul>
        </section>
    </section>

    <section id="FutureWork">
        <h2>Future work</h2>
        <p>We identified three main objectives which can be pursued in order to greatly improve the overall robustness of our project:</p>

        <section id="toDoItem-1">
            <h3>Sentence analysis</h3>
            <ul>
                <li>It would make it easier for us to exclude sentences that are not strongly related to the subject tweet (either heuristics or something else)</li>
                <li>Analyzing individual question would make it easier to detect negations (which we currently don't detect)</li>
            </ul>
        </section>
		
		<section id="toDoItem-2">
            <h3>Semantic relation extraction and evaluation	</h3>
            <ul>
                <li>Useful for occasions when the tweet content is simple and we cannot rely on written articles about the topic</li>
            </ul>
        </section>
		<section id="toDoItem-2">
            <h3>Distributed computing for demanding steps</h3>
            <ul>
                <li>Some steps can benefit from distributed computing, such as web scraping and sentence analysis, at this moment being on services only</li>
            </ul>
        </section>
    </section>

    <section id="finalUsers">
        <h2>Final users</h2>
        <p>This browser extension can be used by any user regardless of age, emphasizing a minimalist design and easy to use. In order to be easy to download, the extension will be added to the market used by Google. A material design was chosen to streamline the user interaction. Also the colors used for the badges on twitter express the visibility of fake news.</p>

        <section id="useCases">
            <h3>Use Cases:</h3>
            <ul>
                <li>User will be able to install the plugin from a store</li>
                <li>User will be able to disable/enable the application.</li>
                <li>User will be able to set the maximum/minimum level of marking a news as fake</li>
                <li>User will be able to see results from a news</li>
                <li>User will be able to see what resources we found</li>
                <li>User will be able to open those resources</li>
				<li>User will be able to give a vote to a post</li>
            </ul>
        </section>
    </section>

    <section id="openApi">
        <h2>OpenAPI</h2>
        <p> The api documentation was generated using swagger preview version which we adapted to the .NET Core 3.0 version. Also the documentation is available <a href="https://fakenewsdetection.azurewebsites.net/swagger/index.html">here.</a> </p>
        <img src="images/Diag_4.png" height="790" width="830" />
    </section>

    <section id="externalSources">
        <h2>External data sources</h2>
        <ul>
            <li>Dbpedia.org</li>
            <li>Twitter API</li>
            <li>Various google search results using scrapper</li>
        </ul>
    </section>
	
	<section typeof="sa:Conclusion" id="Conclusions" role="conclusions">
            <h3>Conclusions:</h3>
            <ul>
                <li>Having fake news detection as primary objective, our proposed method can have decent results when focused on tweets about popular topics that can be verified via written articles found on the web.</li>
				<li>For cases where there are multiple subjects into a article it may take too much time to get all evaluation over ontology, but if it is extracted by the subjects/predicats it will make it faster, but not enough trustful.</li>
                <li>The methods are not suited for tweets that have multiple subjects or predicats due the fact they may put a question mark over the chunks we get for semantic relation extraction and evaluation based on ontology.</li>
            </ul>
        </section>
    </section>

    <section id="biblio-references">
        <h2>References</h2>
        <ul>
            <li>https://github.com/Cisco-Talos/fnc-1</li>
            <li>https://github.com/hanselowski/athene_system</li>
            <li>https://www.kdd.org/exploration_files/19-1-Article2.pdf</li>
            <li>http://ceur-ws.org/Vol-2350/paper10.pdf</li>
            <li>https://arxiv.org/pdf/1901.06437.pdf</li>
            <li>https://www.newsguardtech.com/wp-content/uploads/2019/10/The-Australian.-NewsGuard-shines-a-red-light-on-fake-news-sources-Oct-3-2019.pdf</li>
            <li>https://www.cyberghostvpn.com/privacyhub/fake-news/</li>
            <li>http://ceur-ws.org/Vol-2161/paper5.pdf</li>
            <li>https://arxiv.org/ftp/arxiv/papers/1904/1904.03989.pdf</li>
            <li>https://www.semanticscholar.org/paper/Explainable-Machine-Learning-for-Fake-News-Reis-Correia/b5b96a5295f1f1110d91b7a06e6e4bc8c079ff55</li>
            <li>https://medium.com/newco/how-to-detect-fake-news-in-real-time-9fdae0197bfd</li>
            <li>https://www.w3.org/2001/sw/news</li>
            <li>https://www.factcheck.org/</li>
            <li>https://www.politifact.com/</li>
            <li>https://www.snopes.com/</li>
            <li>http://verificationhandbook.com/downloads/verification.handbook.pdf</li>
            <li>http://fotoforensics.com/</li>
            <li>https://images.google.com/</li>
            <li>https://www.google.com/streetview/</li>
            <li>http://exif.regex.info/exif.cgi</li>
            <li>https://www.tineye.com/</li>
            <li>http://wikimapia.org/#lang=ro&lat=51.514200&lon=-0.093100&z=12&m=w</li>
            <li>https://www.mindtools.com/pages/article/fake-news.htm</li>
            <li>https://en.wikipedia.org/wiki/Fake_news</li>
            <li>http://theconversation.com/the-language-gives-it-away-how-an-algorithm-can-help-us-detect-fake-news-120199</li>
            <li>https://towardsdatascience.com/fake-news-classification-via-anomaly-detection-765c4c71d539</li>
            <li>https://www.newsguardtech.com/</li>
            <li>https://www.bbc.com/news/topics/cjxv13v27dyt/fake-news</li>
            <li>https://fandango-project.eu/the-project/ambition/</li>
            <li>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6762082/</li>
            <li>https://www.econstor.eu/handle/10419/190384</li>
            <li>https://www.econstor.eu/handle/10419/190384</li>
            <li>https://www.researchgate.net/publication/333627738_Semantic_Fake_News_Detection_A_Machine_Learning_Perspective</li>
            <li>https://link.springer.com/chapter/10.1007/978-3-642-12630-7_16</li>
            <li>https://www.researchgate.net/publication/221435652_Personalized_Semantic_News_Combining_Semantics_and_Television</li>
            <li>https://www.bbc.co.uk/academy/en/articles/art20130724121658626</li>
            <li>https://www.bbc.co.uk/blogs/bbcinternet/2010/02/case_study_use_of_semantic_web.html</li>
            <li>https://libguides.lmu.edu/c.php?g=595781&p=4121899</li>
            <li>https://www.theverge.com/2018/8/23/17383912/fake-news-browser-plug-ins-ai-information-apocalypse</li>
            <li>https://rawinfopages.co.uk/add-a-fake-news-detector-to-your-browser-to-alert-you-to-problems/</li>
        </ul>
    </section>

</body>

</html>